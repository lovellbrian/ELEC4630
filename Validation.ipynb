{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Validation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPo1HwNW+66e3iJG9SVYYa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lovellbrian/ELEC4630/blob/master/Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT4CVK5I1E-R"
      },
      "source": [
        "Chollet P98\n",
        "\n",
        "SIMPLE HOLD-OUT VALIDATION\n",
        "Set apart some fraction of your data as your test set. Train on the remaining data, and\n",
        "evaluate on the test set. As you saw in the previous sections, in order to prevent information\n",
        "leaks, you shouldn’t tune your model based on the test set, and therefore you\n",
        "should also reserve a validation set.\n",
        "Schematically, hold-out validation looks like figure 4.1. The following listing shows\n",
        "a simple implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa51Gk_R0__y"
      },
      "source": [
        "num_validation_samples = 10000\n",
        "np.random.shuffle(data)\n",
        "validation_data = data[:num_validation_samples]\n",
        "data = data[num_validation_samples:]\n",
        "training_data = data[:]\n",
        "model = get_model()\n",
        "model.train(training_data)\n",
        "validation_score = model.evaluate(validation_data)\n",
        "# At this point you can tune your model,\n",
        "# retrain it, evaluate it, tune it again...\n",
        "model = get_model()\n",
        "model.train(np.concatenate([training_data,\n",
        "validation_data]))\n",
        "test_score = model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "144GOOMk1S5K"
      },
      "source": [
        "K-FOLD VALIDATION\n",
        "With this approach, you split your data into K partitions of equal size. For each partition\n",
        "i, train a model on the remaining K – 1 partitions, and evaluate it on partition i.\n",
        "Your final score is then the averages of the K scores obtained. This method is helpful\n",
        "when the performance of your model shows significant variance based on your traintest\n",
        "split. Like hold-out validation, this method doesn’t exempt you from using a distinct\n",
        "validation set for model calibration.\n",
        "Schematically, K-fold cross-validation looks like figure 4.2. Listing 4.2 shows a simple\n",
        "implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPee_Geu1c3Z"
      },
      "source": [
        "k = 4\n",
        "num_validation_samples = len(data) // k\n",
        "np.random.shuffle(data)\n",
        "validation_scores = []\n",
        "for fold in range(k):\n",
        "  validation_data = data[num_validation_samples * fold:\n",
        "  num_validation_samples * (fold + 1)]\n",
        "  training_data = data[:num_validation_samples * fold] +\n",
        "  data[num_validation_samples * (fold + 1):]\n",
        "  model = get_model()\n",
        "  model.train(training_data)\n",
        "  validation_score = model.evaluate(validation_data)\n",
        "  validation_scores.append(validation_score)\n",
        "  validation_score = np.average(validation_scores)\n",
        "  model = get_model()\n",
        "  model.train(data)\n",
        "  test_score = model.evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8pL-gD8117C"
      },
      "source": [
        "ITERATED K-FOLD VALIDATION WITH SHUFFLING\n",
        "This one is for situations in which you have relatively little data available and you need\n",
        "to evaluate your model as precisely as possible. I’ve found it to be extremely helpful in\n",
        "Kaggle competitions. It consists of applying K-fold validation multiple times, shuffling\n",
        "the data every time before splitting it K ways. The final score is the average of the\n",
        "scores obtained at each run of K-fold validation. Note that you end up training and\n",
        "evaluating P × K models (where P is the number of iterations you use), which can very\n",
        "expensive."
      ]
    }
  ]
}